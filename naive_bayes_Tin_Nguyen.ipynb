{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_bayes_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGhv8hpnmmRJ"
      },
      "source": [
        "def create_Dictionary(root_dir):                                     #define create_Dictionary to look for root_dir\r\n",
        "  all_words = []                                                     #make all_words to be a list\r\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]  #make emails is the list. findind root_dir, then combine into complete path with f is define in the list of root_dir\r\n",
        "  for text in emails:                                                #create a loop for text in emails, add the conditions below\r\n",
        "    with open(text) as m:                                            #open texts in email\r\n",
        "      for line in m:                                                 \r\n",
        "        words = line.split()                                         #break the line text into list of words or numbers\r\n",
        "        all_words += words                                           #Add the single word with the single number\r\n",
        "  dictionary = Counter(all_words)                                    #count the elements in the strings\r\n",
        "  unused_emails = list(dictionary)                                   #create the list with words and number of frequecy\r\n",
        "\r\n",
        "  for item in unused_emails:                                         #create the loop for unused emails to add the conditions\r\n",
        "    if item.isalpha() == False:                                      #apply condition if the time is not aplhabic letters\r\n",
        "      del dictionary[item]                                           #delete if the condition is true\r\n",
        "    elif len(item) == 1:                                             #apply condition if the word length of item is 1\r\n",
        "      del dictionary[item]                                           #delete if the condition is true\r\n",
        "  dictionary = dictionary.most_common(3000)                          #define the dictionary where we use only 3000 common words in dictionary\r\n",
        "  return dictionary"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu7wb8L_XF0p"
      },
      "source": [
        "The codes are used to clean up the data. By breaking down the words in the emails, we can count the frequency of the words. Each word in the email is separated for the count. The result has the count for each word. Moreover, the non-alphabetical word will be removed as well. Then, when the dictionary is run, the word with the frequency that is lower than 3000 will be removed. The final result will have the list of the common words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 columns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "def extract_features(mail_dir):                                                 #define extract_features to create loops that apply on mail_dir               \n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]            #make files is the list. findind mail_dir, then combine into complete path with fi is define in the list of mail_dir\n",
        "  features_matrix = np.zeros((len(files),3000))                                 #define features_matrix with number of rows as length of files and columns to be 3000. For example, function returns ([[0,0],..]))\n",
        "  train_labels = np.zeros(len(files))                                           #create the template for train_labels with the length of files represent the numbers of columns\n",
        "  count = 1;                                                                    \n",
        "  docID = 0;                                                                    \n",
        "  for fil in files:                                                             #create loop for fil in files to add conditions\n",
        "    with open(fil) as fi:                                                       #open fil when fi meets conditions\n",
        "      for i, line in enumerate(fi):                                             #use enumerate to add object and count\n",
        "        if i ==2:                                                               \n",
        "          words = line.split()                                                  #split into separate line when words equal 2\n",
        "          for word in words:                                                    #add conditions for word in words\n",
        "            wordID = 0                                                          #wordID starts with 0\n",
        "            for i, d in enumerate(dictionary):                            \n",
        "              if d[0] == word:                                                  #create the labels of name of columns\n",
        "                wordID = i                                                      \n",
        "                features_matrix[docID,wordID] = words.count(word)               #count the word in words, then apply the provide the label and count for each word\n",
        "      train_labels[docID] = 0;\n",
        "      filepathTokens = fil.split('/')                                           \n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      if lastToken.startswith(\"spmsg\"):                                         #define the spam message and place them in different column\n",
        "        train_labels[docID] = 1;\n",
        "        count = count + 1\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels              "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFCxc0aCXCFi"
      },
      "source": [
        "The codes first create the templates of the table by using the length of files to be the number of rows, and number of columns with 3000. Each word will be created with the count (frequency) with it. The codes also apply the feature to separate the spam message. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "source": [
        "TRAIN_DIR = '/content/drive/MyDrive/MSBA_Colab_2020/ML_Algorithms/Naive_Bayes/train-mails'\n",
        "TEST_DIR = '/content/drive/MyDrive/MSBA_Colab_2020/ML_Algorithms/Naive_Bayes/test-mails'"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "cb516f20-0585-42cc-9690-c742f8a1dae3"
      },
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7_cL6MhXBBS"
      },
      "source": [
        "From the accuracy of 96.15%, the Naive Bayes show the independent relationships between words. Based on the high freqency words, they do not have much relationship to other words. When one word appears in that email, there will not be another word that always in the same sentence; the other words will be random. The system suggests that we cannot predict the outcome of having set of words that always go together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lOPNkugK4Zz",
        "outputId": "5bac9bb4-0824-4bc5-d027-1cc5ee148f5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}